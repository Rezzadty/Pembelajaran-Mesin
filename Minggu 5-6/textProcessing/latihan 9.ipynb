{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nama     : Reza Aditya Prabowo\n",
    "# NIM      : A11.2022.14201\n",
    "# Kelompok : A11.4419\n",
    "\n",
    "# Praktek Processing\n",
    "# =============================\n",
    "# ***Universitas Dian Nuswantoro***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Library\n",
    "import re\n",
    "import string\n",
    "import time\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset \n",
    "# Dataset ini menggunakan settimen dan emosi \n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Dataset_Sentimen_Emosi.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(['Emosi'], axis=1)  # Menghapus kolom 'Emosi' dari dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing dataset\n",
    "# Menggunakan modul AMS 01-03\n",
    "from ekphrasis.classes.preprocessor import TextPreProcessor\n",
    "from ekphrasis.classes.tokenizer import SocialTokenizer\n",
    "from ekphrasis.dicts.emoticons import emoticons\n",
    "\n",
    "# Inisialisasi TextPreProcessor dari ekphrasis untuk prapemrosesan teks\n",
    "text_processor = TextPreProcessor(\n",
    "    # terms that will be normalized\n",
    "    normalize=['email', 'percent', 'money', 'phone', 'user',\n",
    "        'time', 'date', 'number'],\n",
    "    # terms that will be annotated\n",
    "    # annotate={\"hashtag\", \"allcaps\", \"elongated\", \"repeated\",'emphasis', 'censored'},\n",
    "    annotate={\"hashtag\"},\n",
    "    fix_html=True,  # Memperbaiki token HTML\n",
    "    \n",
    "    # corpus from which the word statistics are going to be used \n",
    "    # for word segmentation \n",
    "    segmenter=\"twitter\", \n",
    "    \n",
    "    # corpus from which the word statistics are going to be used \n",
    "    # for spell correction\n",
    "    corrector=\"twitter\", \n",
    "    \n",
    "    unpack_hashtags=True,  # Melakukan segmentasi kata pada hashtag\n",
    "    unpack_contractions=True,  # Membuka kontraksi (contoh: can't -> can not)\n",
    "    spell_correct_elong=False,  # Koreksi ejaan untuk kata yang diperpanjang\n",
    "    \n",
    "    # Pilih tokenizer. Bisa menggunakan SocialTokenizer, atau menggunakan tokenizer sendiri\n",
    "    tokenizer=SocialTokenizer(lowercase=True).tokenize,\n",
    "    \n",
    "    # Daftar kamus, untuk mengganti token yang diekstraksi dari teks\n",
    "    dicts=[emoticons]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fungsi untuk membersihkan data menggunakan ekphrasis\n",
    "def bersih_data(text):\n",
    "    return \" \".join(text_processor.pre_process_doc(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fungsi untuk menghilangkan karakter non-ASCII\n",
    "def non_ascii(text):\n",
    "    return text.encode('ascii', 'replace').decode('ascii')\n",
    "\n",
    "# Fungsi untuk menghilangkan spasi berlebih\n",
    "def remove_space_alzami(text):\n",
    "    return \" \".join(text.split())\n",
    "\n",
    "# Fungsi untuk menghilangkan emoji\n",
    "def remove_emoji_alzami(text):\n",
    "    return ' '.join(re.sub(\"([x#][A-Za-z0-9]+)\", \" \", text).split())\n",
    "\n",
    "# Fungsi untuk menghilangkan karakter tab\n",
    "def remove_tab(text):\n",
    "    return text.replace('\\\\t', \" \").replace('\\\\n', \" \").replace('\\\\u', \" \").replace('\\\\', \"\")\n",
    "\n",
    "# Fungsi untuk menghilangkan spasi berlebih\n",
    "def remove_tab2(text):\n",
    "    return re.sub('\\s+', ' ', text)\n",
    "\n",
    "# Fungsi untuk menghilangkan 'RT' dari teks\n",
    "def remove_rt(text):\n",
    "    return text.replace('RT', \" \")\n",
    "\n",
    "# Fungsi untuk menghilangkan mention (@)\n",
    "def remove_mention(text):\n",
    "    return ' '.join(re.sub(\"([@#][A-Za-z0-9]+)|(\\w+:\\/\\/\\S+)\", \" \", text).split())\n",
    "\n",
    "# Fungsi untuk menghilangkan URL yang tidak lengkap\n",
    "def remove_incomplete_url(text):\n",
    "    return text.replace(\"http://\", \" \").replace(\"https://\", \" \")\n",
    "\n",
    "# Fungsi untuk menghilangkan karakter tunggal\n",
    "def remove_single_char(text):\n",
    "    return re.sub(r\"\\b[a-zA-Z]\\b\", \"\", text)\n",
    "\n",
    "# Fungsi untuk menghilangkan titik berlebih\n",
    "def remove_excessive_dot(text):\n",
    "    return text.replace('..', \" \")\n",
    "\n",
    "# Fungsi untuk mengganti karakter strip dengan spasi\n",
    "def change_stripe(text):\n",
    "    return text.replace('-', \" \")\n",
    "\n",
    "# Fungsi untuk mengubah teks menjadi huruf kecil\n",
    "def lower(text):\n",
    "    return text.lower()\n",
    "\n",
    "# Fungsi untuk menghilangkan whitespace di awal dan akhir teks\n",
    "def remove_whitespace_LT(text):\n",
    "    return text.strip()\n",
    "\n",
    "# Fungsi untuk menghilangkan spasi berlebih di antara kata\n",
    "def remove_whitespace_multiple(text):\n",
    "    return re.sub('\\s+', ' ', text)\n",
    "\n",
    "# Fungsi untuk menghilangkan tanda baca kecuali underscore (_)\n",
    "def remove_punctuation(text):\n",
    "    remove = string.punctuation\n",
    "    remove = remove.replace(\"_\", \"\")  # Jangan menghapus underscore\n",
    "    pattern = r\"[{}]\".format(remove)  # Buat pola\n",
    "    return re.sub(pattern, \"\", text) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fungsi untuk menghapus teks \"<number>\"\n",
    "def remove_number_eks(text):\n",
    "    return text.replace('<number>', \" \")\n",
    "\n",
    "# Fungsi untuk menghilangkan angka\n",
    "def remove_angka(text):\n",
    "    return re.sub(r\"\\d+\", \"\", text) \n",
    "\n",
    "# Fungsi untuk menghapus teks \"URL\"\n",
    "def remove_URL_eks(text):\n",
    "    return text.replace('URL', \" \").replace('url', \" \")\n",
    "\n",
    "# Fungsi untuk menambahkan spasi sebelum dan setelah tanda baca\n",
    "def space_punctuation(text):\n",
    "    return re.sub('(?<! )(?=[.,!?()])|(?<=[.,!?()])(?! )', r' ', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Melakukan pembersihan dengan memanggil fungsi yang didefinisikan di atas\n",
    "i = 0\n",
    "final_string = []\n",
    "s = \"\"\n",
    "for text in df['Tweet'].values:\n",
    "    filteredSentence = []\n",
    "    EachReviewText = \"\"\n",
    "    proc = remove_rt(text)\n",
    "    proc = lower(proc)\n",
    "    proc = change_stripe(proc)\n",
    "    proc = remove_emoji_alzami(proc)\n",
    "    proc = remove_tab(proc)\n",
    "    proc = remove_tab2(proc)\n",
    "    proc = non_ascii(proc)\n",
    "    proc = remove_incomplete_url(proc)\n",
    "    proc = remove_excessive_dot(proc)\n",
    "    proc = remove_whitespace_LT(proc)\n",
    "    proc = remove_whitespace_multiple(proc)\n",
    "    proc = remove_single_char(proc)\n",
    "    proc = space_punctuation(proc)\n",
    "    proc = remove_punctuation(proc)\n",
    "    proc = remove_space_alzami(proc)\n",
    "    proc = bersih_data(proc)\n",
    "    proc = remove_number_eks(proc)\n",
    "    proc = remove_angka(proc) \n",
    "    proc = remove_URL_eks(proc)\n",
    "    EachReviewText = proc\n",
    "    final_string.append(EachReviewText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"step01\"] = final_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hapus Data Kosong\n",
    "df_hapus = df[~df['step01'].str.contains(\" \")]  # Memilih data yang tidak kosong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hapus.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hapus.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new = df[~df.isin(df_hapus)].dropna()  # Menghapus data kosong dari dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nomalisasi kata slang menjadi kata baku menggunakan token\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fungsi untuk tokenisasi teks\n",
    "def word_tokenize_wrapper(text):\n",
    "  return word_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new['tokens'] = df['step01'].apply(word_tokenize_wrapper)  # Tokenisasi teks pada kolom 'step01'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Membaca kamus kata slang dan kata baku\n",
    "normalized_word = pd.read_csv('kamus_clean.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Membuat dictionary untuk normalisasi kata\n",
    "normalized_word_dict = {}\n",
    "for index, row in normalized_word.iterrows():\n",
    "    if row[0] not in normalized_word_dict:\n",
    "        normalized_word_dict[row[0]] = row[1] \n",
    "\n",
    "# Fungsi untuk normalisasi istilah\n",
    "def normalized_term(document):\n",
    "    return [normalized_word_dict[term] if term in normalized_word_dict else term for term in document]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new['final_tokens'] = df_new['tokens'].apply(normalized_term)  # Menerapkan normalisasi istilah"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=0\n",
    "final_string_tokens = []\n",
    "for text in df_new['final_tokens'].values:\n",
    "    EachReviewText = \"\"\n",
    "    EachReviewText = ' '.join(text)\n",
    "    final_string_tokens.append(EachReviewText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new[\"step02\"] = final_string_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Menyimpan dataset yang sudah dibersihkan ke file CSV\n",
    "df_new.to_csv('clean_dataset.csv', sep=\";\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
