{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nama     : Reza Aditya Prabowo\n",
    "# NIM      : A11.2022.14201\n",
    "# Kelompok : A11.4419\n",
    "\n",
    "# Praktek Processing NLTK dan Spacy\n",
    "# =============================\n",
    "# ***Universitas Dian Nuswantoro***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLTK meruapakan string processing library yang dimana dia menerima input string dan mengembalikan string tersebut atau membuat oupot list dari string tersebut\n",
    "# Spacy mengsuport vector yang dimana NLTK tidak\n",
    "# yang dimana pada tokenization dan POS-Tagging Spacy berforma lebih baik, tapi pada sentence Tokenization NLTK lebih baik\n",
    "# NLTK mengsupport berbagai macam bahasa yang dimana spacy hanya 7 bahasa\n",
    "# token\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_tokenize_wrapper(text):\n",
    "  return word_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "teks_nltk = word_tokenize_wrapper('Yahya beserta teman-teman TK suka melihat lumba-lumba di Batang Dolphin Center')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "teks_nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Menggunakan Spacy \n",
    "from spacy.lang.id import Indonesian\n",
    "import spacy\n",
    "\n",
    "nlp = Indonesian()  # use directly\n",
    "nlp = spacy.blank('id')  # blank instance'\n",
    "Teks = nlp('Yahya beserta teman-teman TK suka melihat lumba-lumba di Batang Dolphin Center')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Token_kata = [token.text for token in Teks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Token_kata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mengload Library \n",
    "import re\n",
    "import string\n",
    "import time\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Melakukan load dataset\n",
    "# Pada dataset ini menggunakan sentimen dan emosi, menggunakan sentimen terlebih dahulu \n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Dataset_Sentimen_Emosi.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(['Emosi'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Menggunakan modul AMS 01-03\n",
    "\n",
    "from ekphrasis.classes.preprocessor import TextPreProcessor\n",
    "from ekphrasis.classes.tokenizer import SocialTokenizer\n",
    "from ekphrasis.dicts.emoticons import emoticons\n",
    "\n",
    "text_processor = TextPreProcessor(\n",
    "    # terms that will be normalized\n",
    "    normalize=['email', 'percent', 'money', 'phone', 'user',\n",
    "        'time', 'date', 'number'],\n",
    "    # terms that will be annotated\n",
    "    #annotate={\"hashtag\", \"allcaps\", \"elongated\", \"repeated\",'emphasis', 'censored'},\n",
    "    annotate={\"hashtag\"},\n",
    "    fix_html=True,  # fix HTML tokens\n",
    "    \n",
    "    # corpus from which the word statistics are going to be used \n",
    "    # for word segmentation \n",
    "    segmenter=\"twitter\", \n",
    "    \n",
    "    # corpus from which the word statistics are going to be used \n",
    "    # for spell correction\n",
    "    corrector=\"twitter\", \n",
    "    \n",
    "    unpack_hashtags=True,  # perform word segmentation on hashtags\n",
    "    unpack_contractions=True,  # Unpack contractions (can't -> can not)\n",
    "    spell_correct_elong=False,  # spell correction for elongated words\n",
    "    \n",
    "    # select a tokenizer. You can use SocialTokenizer, or pass your own\n",
    "    # the tokenizer, should take as input a string and return a list of tokens\n",
    "    tokenizer=SocialTokenizer(lowercase=True).tokenize,\n",
    "    \n",
    "    # list of dictionaries, for replacing tokens extracted from the text,\n",
    "    # with other expressions. You can pass more than one dictionaries.\n",
    "    dicts=[emoticons]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# panggil ekphrasis\n",
    "\n",
    "def bersih_data(text):\n",
    "    return \" \".join(text_processor.pre_process_doc(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fungsi dari AMS 01-03. silakan cek bagaimana saya merubah menjadi fungsi\n",
    "\n",
    "def non_ascii(text):\n",
    "    return text.encode('ascii', 'replace').decode('ascii')\n",
    "\n",
    "def remove_space_alzami(text):\n",
    "    return \" \".join(text.split())\n",
    "\n",
    "def remove_emoji_alzami(text):\n",
    "    return ' '.join(re.sub(\"([x#][A-Za-z0-9]+)\",\" \", text).split())\n",
    "\n",
    "def remove_tab(text):\n",
    "    return text.replace('\\\\t',\" \").replace('\\\\n',\" \").replace('\\\\u',\" \").replace('\\\\',\"\")\n",
    "\n",
    "def remove_tab2(text):\n",
    "    return re.sub('\\s+',' ',text)\n",
    "\n",
    "def remove_rt(text):\n",
    "    return text.replace('rt',\" \")\n",
    "\n",
    "def remove_mention(text):\n",
    "    return ' '.join(re.sub(\"([@#][A-Za-z0-9]+)|(\\w+:\\/\\/\\S+)\",\" \", text).split())\n",
    "\n",
    "def remove_incomplete_url(text):\n",
    "    return text.replace(\"http://\", \" \").replace(\"https://\", \" \")\n",
    "\n",
    "def remove_single_char(text):\n",
    "    return re.sub(r\"\\b[a-zA-Z]\\b\", \"\", text)\n",
    "\n",
    "def remove_excessive_dot(text):\n",
    "    return text.replace('..',\" \")\n",
    "\n",
    "def change_stripe(text):\n",
    "    return text.replace('-',\" \")\n",
    "\n",
    "def lower(text):\n",
    "    return text.lower()\n",
    "\n",
    "def remove_single_char(text):\n",
    "    return re.sub(r\"\\b[a-zA-Z]\\b\", \"\", text)\n",
    "\n",
    "def remove_excessive_dot(text):\n",
    "    return text.replace('..',\" \")\n",
    "\n",
    "def lower(text):\n",
    "    return text.lower()\n",
    "\n",
    "def remove_whitespace_LT(text):\n",
    "    return text.strip()\n",
    "\n",
    "def remove_whitespace_multiple(text):\n",
    "    return re.sub('\\s+',' ',text)\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    remove = string.punctuation\n",
    "    remove = remove.replace(\"_\", \"\") # don't remove hyphens\n",
    "    pattern = r\"[{}]\".format(remove) # create the pattern\n",
    "    return re.sub(pattern, \"\", text) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hapus untuk <>\n",
    "def remove_number_eks(text):\n",
    "    return text.replace('<number>',\" \")\n",
    "\n",
    "def remove_angka(text):\n",
    "    return re.sub(r\"\\d+\", \"\", text) \n",
    "\n",
    "def remove_URL_eks(text):\n",
    "    return text.replace('URL',\" \").replace('url',\" \")\n",
    "\n",
    "def space_punctuation(text):\n",
    "    return re.sub('(?<! )(?=[.,!?()])|(?<=[.,!?()])(?! )', r' ', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Melakukan pembersihan dengan memanggil fungsi dan didefinisikan diatas \n",
    "i = 0\n",
    "final_string = []\n",
    "s = \"\"\n",
    "for text in df['Tweet'].values:\n",
    "    filteredSentence = []\n",
    "    EachReviewText = \"\"\n",
    "    proc = lower(text)\n",
    "    proc = change_stripe(text)\n",
    "    proc = remove_emoji_alzami(proc)\n",
    "    proc = remove_tab(proc)\n",
    "    proc = remove_tab2(proc)\n",
    "    proc = non_ascii(proc)\n",
    "    proc = remove_incomplete_url(proc)\n",
    "    proc = remove_excessive_dot(proc)\n",
    "    proc = remove_whitespace_LT(proc)\n",
    "    proc = remove_whitespace_multiple(proc)\n",
    "    proc = remove_single_char(proc)\n",
    "    proc = space_punctuation(proc)\n",
    "    proc = remove_punctuation(proc)\n",
    "    proc = remove_space_alzami(proc)\n",
    "    proc = bersih_data(proc)\n",
    "    #proc = remove_rt(proc)\n",
    "    proc = remove_number_eks(proc)\n",
    "    proc = remove_angka(proc) \n",
    "    proc = remove_URL_eks(proc)\n",
    "    EachReviewText = proc\n",
    "    final_string.append(EachReviewText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"step01\"] = final_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Menghapus data kosong \n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hapus = df[~df['step01'].str.contains(\" \")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hapus.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hapus.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new = df[~df.isin(df_hapus)].dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalisasi kata slang menjadi kata baku \n",
    "# token yang digunakan\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_tokenize_wrapper(text):\n",
    "  return word_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new['tokens'] = df['step01'].apply(word_tokenize_wrapper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_word = pd.read_csv('kamus_clean.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_word_dict = {}\n",
    "\n",
    "for index, row in normalized_word.iterrows():\n",
    "    if row[0] not in normalized_word_dict:\n",
    "        normalized_word_dict[row[0]] = row[1] \n",
    "\n",
    "def normalized_term(document):\n",
    "    return [normalized_word_dict[term] if term in normalized_word_dict else term for term in document]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new['final_tokens'] = df_new['tokens'].apply(normalized_term)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=0\n",
    "final_string_tokens = []\n",
    "for text in df_new['final_tokens'].values:\n",
    "    EachReviewText = \"\"\n",
    "    EachReviewText = ' '.join(text)\n",
    "    final_string_tokens.append(EachReviewText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new[\"step02\"] = final_string_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mengsimpan data\n",
    "df.to_csv('clean_dataset2.csv',sep=\";\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
